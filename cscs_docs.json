[
  {
    "doc_id": 0,
    "file_path": "index.md",
    "file_name": "index.md",
    "title": "CSCS Documentation",
    "content": "# CSCS Documentation\n\nThe Alps Research Infrastructure hosts multiple platforms and clusters targeting different communities.\nA good spot to get started is with the documentation for the platform that your project is running on.\n\n<div class=\"grid cards\" markdown>\n\n-   :fontawesome-solid-layer-group: __Platforms__\n\n    Projects at CSCS are granted access to [clusters][ref-alps-clusters], which are managed by platforms.\n\n    [:octicons-arrow-right-24: HPC Platform (__Daint__, __Eiger__)][ref-platform-hpcp]\n\n    [:octicons-arrow-right-24: Machine Learning Platform (__Clariden__, __Bristen__)][ref-platform-mlp]\n\n    [:octicons-arrow-right-24: Climate and Weather Platform (__Santis__)][ref-platform-cwp]\n\n    For an overview of the different platforms:\n\n    [:octicons-arrow-right-24: Platforms overview][ref-alps-platforms]\n\n</div>\n\nAlps is a general-purpose compute and data Research Infrastructure (RI) open to the broad community of researchers in Switzerland and the rest of the world.\nFind out more about Alps...\n\n<div class=\"grid cards\" markdown>\n\n-   :fontawesome-solid-mountain-sun: __Alps__\n\n    Learn more about the Alps research infrastructure\n\n    [:octicons-arrow-right-24: Alps Overview][ref-alps]\n\n    Get detailed information about the main components of the infrastructure\n\n    [:octicons-arrow-right-24: Alps Clusters][ref-alps-clusters]\n\n    [:octicons-arrow-right-24: Alps Hardware][ref-alps-hardware]\n\n    [:octicons-arrow-right-24: Alps Storage][ref-alps-storage]\n\n-   :fontawesome-solid-key: __Logging In__\n\n    Once you have an account, you can set up multi factor authentication:\n\n    [:octicons-arrow-right-24: Setting up MFA][ref-mfa]\n\n    Then access CSCS services:\n\n    [:octicons-arrow-right-24: Accessing CSCS Web Services][ref-access-web]\n\n    [:octicons-arrow-right-24: Using SSH][ref-ssh]\n\n    [:octicons-arrow-right-24: FirecREST API][ref-firecrest]\n\n</div>\n<div class=\"grid cards\" markdown>\n\n-   :fontawesome-solid-layer-group: __Accounts and Projects__\n\n    If you are a new user, or working on a project for the first time:\n\n    [:octicons-arrow-right-24: Accounts and Projects][ref-account-management]\n\n</div>\n\n<div class=\"grid cards\" markdown>\n-   <p style=\"text-align:center\">Visit <a href=\"https://status.cscs.ch/\">status.cscs.ch</a> for the status of Alps systems, and the latest announcements.</p>\n</div>\n\n\n## Tutorials and Guides\n\nLearn by doing with our guides and tutorials.\n\n<div class=\"grid cards\" markdown>\n-   :fontawesome-solid-layer-group: __Tutorials__\n\n    Hands on tutorials that show how to implement workflows on Alps.\n\n    [:octicons-arrow-right-24: Machine Learning][ref-tutorials-ml]\n\n-   :fontawesome-solid-mountain-sun: __Guides__\n\n    Guides with practical advice, hints and tips for key topics.\n\n    [:octicons-arrow-right-24: Using storage effectively][ref-guides-storage]\n\n    [:octicons-arrow-right-24: Accessing internet and external services][ref-guides-internet-access]\n\n</div>\n\n## Tools and Services\n\n<div class=\"grid cards\" markdown>\n-   :fontawesome-solid-hammer: __Services__\n\n    CSCS provides services and software on Alps.\n\n    [:octicons-arrow-right-24: Services Overview](services/index.md)\n\n    Learn about individual services\n\n    [:octicons-arrow-right-24: Developer Portal][ref-devportal]\n\n    [:octicons-arrow-right-24: CI/CD for external projects](services/cicd.md)\n\n\n-   :fontawesome-solid-hammer: __Software__\n\n    CSCS provides applications, programming environments and tools.\n\n    [:octicons-arrow-right-24: Scientific applications](software/sciapps/index.md)\n\n    [:octicons-arrow-right-24: Programming environments](software/prgenv/index.md)\n\n    [:octicons-arrow-right-24: Development tools](software/devtools/index.md)\n\n    Find out how to use uenv and containers to access software\n\n    [:octicons-arrow-right-24: uenv][ref-uenv]\n\n    [:octicons-arrow-right-24: Container engine][ref-container-engine]\n\n\n-   :fontawesome-solid-screwdriver-wrench: __Data management and storage__\n\n    CSCS provides [data management and storage](storage/index.md) services\n\n    [:octicons-arrow-right-24: File systems](storage/filesystems.md)\n\n    [:octicons-arrow-right-24: Data transfer](storage/transfer.md)\n\n    [:octicons-arrow-right-24: Long term storage](storage/longterm.md)\n\n    [:octicons-arrow-right-24: Object storage](storage/object.md)\n\n</div>\n\n[](){#ref-get-in-touch}\n## Get in Touch\n\nIf you cannot find the information that you need in the documentation, help is available.\n\n<div class=\"grid cards\" markdown>\n\n-   :fontawesome-solid-headset: __Get Help__\n\n    Contact the CSCS Service Desk for help.\n\n    [:octicons-arrow-right-24: Service Desk](https://jira.cscs.ch/plugins/servlet/desk)\n\n-   :fontawesome-regular-comments: __Chat__\n\n    Discuss Alps with other users and CSCS staff on Slack.\n\n    [:octicons-arrow-right-24: CSCS User Slack](https://cscs-users.slack.com/)\n\n<div class=\"grid cards\" markdown>\n-   :fontawesome-solid-hammer: __Contribute__\n\n    The source for the documentation is hosted on GitHub.\n\n    [:octicons-arrow-right-24: Contribute to the docs ](contributing/index.md)\n</div>\n\n</div>\n\n",
    "source": "cscs-docs"
  },
  {
    "doc_id": 1,
    "file_path": "access\\firecrest.md",
    "file_name": "firecrest.md",
    "title": "FirecREST",
    "content": "[](){#ref-firecrest}\n# FirecREST\n\nFirecREST is a RESTful API for programmatically accessing High-Performance Computing resources, developed at CSCS.\n\nUsers can make use of FirecREST to automate access to HPC, enabling [CI/CD pipelines](https://eth-cscs.github.io/firecrest-v2/use_cases/CI-pipeline/), [workflow orchestrators](https://eth-cscs.github.io/firecrest-v2/use_cases/workflow-orchestrator/), and other tools against HPC resources.\n\nAdditionally, scientific platform developers can integrate FirecREST into [web-enabled portals](https://eth-cscs.github.io/firecrest-ui/home/) and [web UI applications](https://eth-cscs.github.io/firecrest-v2/use_cases/UI-client-credentials/), allowing them to securely access authenticated and authorized CSCS services such as job submission and data transfer on HPC systems.\n\nUsers can make HTTP requests to perform the following operations:\n\n* basic system utilities like `ls`, `mkdir`, `mv`, `chmod`, `chown`, among others\n* actions against the Slurm workload manager (submit, query, and cancel jobs of the user)\n* internal (between CSCS systems) and external (to/from CSCS systems) data transfers\n\n## FirecREST versions\n\nStarting early 2025, CSCS has introduced a new version of the API: [FirecREST version 2](https://eth-cscs.github.io/firecrest-v2).\n\nVersion 2 is faster, easier to use, and more efficient in resource management than its predecessor, therefore, if you are new to FirecREST start directly using **version 2**.\n\n!!! warning \"Deprecation notice\"\n    If you're using **version 1**, we recommend you to port your applications to use the new version.\n    We will communicate soon the exact date of the decommissioning of version 1 (not before Quarter 4 of 2025).\n\n=== \"Version 2\"\n\n    For a full feature set, have a look at the latest [FirecREST version 2 API specification](https://eth-cscs.github.io/firecrest-v2/openapi) deployed at CSCS.\n\n    Please refer to the [FirecREST-v2 documentation](https://eth-cscs.github.io/firecrest-v2/user_guide/) for detailed documentation.\n\n=== \"Version 1\"\n\n    For a full feature set, have a look at the latest [FirecREST version 1 API specification](https://firecrest-docs.v1.svc.cscs.ch/) deployed at CSCS.\n\n    Please refer to the [FirecREST-v1 documentation](https://firecrest.readthedocs.io/en/latest/) for detailed documentation.\n\n\n## FirecREST Deployment on Alps\n\nFirecREST is available for all three major [Alps platforms][ref-alps-platforms], with a different API endpoint and versions for each platform.\n\n<table>\n<tr><th>Platform</th><th>Version</th><th>API Endpoint</th><th>Clusters</th></tr>\n<tr><td style=\"vertical-align: middle;\" rowspan=\"2\">HPC Platform</td><td>v1</td><td>https://api.cscs.ch/hpc/firecrest/v1</td><td style=\"vertical-align: middle;\" rowspan=\"2\"><a href=\"../../clusters/daint\">Daint</a>, <a href=\"../../clusters/eiger\">Eiger</a></td></tr>\n<tr>                                 <td>v2</td><td>https://api.cscs.ch/hpc/firecrest/v2</td></tr>\n<tr><td style=\"vertical-align: middle;\" rowspan=\"2\">ML Platform</td><td>v1</td><td>https://api.cscs.ch/ml/firecrest/v1</td><td style=\"vertical-align: middle;\" rowspan=\"2\"><a href=\"../../clusters/bristen\">Bristen</a>, <a href=\"../../clusters/clariden\">Clariden</a></td></tr>\n<tr>                                 <td>v2</td><td>https://api.cscs.ch/ml/firecrest/v2</td></tr>\n<tr><td style=\"vertical-align: middle;\" rowspan=\"2\">CW Platform</td><td>v1</td><td>https://api.cscs.ch/cw/firecrest/v1</td><td style=\"vertical-align: middle;\" rowspan=\"2\"><a href=\"../../clusters/santis\">Santis</a></td></tr>\n<tr><td>v2</td><td>https://api.cscs.ch/cw/firecrest/v2</td></tr>\n</table>\n\n\n## Accessing FirecREST\n\n### Clients and access tokens\n\nFor authenticating requests to FirecREST, [client applications][ref-devportal-application] use an **access token** instead of directly using the user's credentials.\nThe access token is a signed JSON Web Token ([JWT](https://jwt.io/introduction)) which contains user information and is only valid for a short time (5 minutes).\nBehind the API, all commands launched by the client will use the account of the user that registered the client, inheriting their access rights.\n\nEvery client has a client ID (Consumer Key) and a secret (Consumer Secret) that are used to get a short-lived access token with an HTTP request.\n\n??? example \"`curl` call to fetch the access token\"\n    ```\n    curl -s -X POST https://auth.cscs.ch/auth/realms/firecrest-clients/protocol/openid-connect/token \\\n         --data \"grant_type=client_credentials\" \\\n         --data \"client_id=<client_id>\" \\\n         --data \"client_secret=<client_secret>\"\n    ```\n\nYou can manage your client application on the [CSCS Developer Portal][ref-devportal].\n\n\nTo use your client credentials to access FirecREST, follow the [API documentation](https://eth-cscs.github.io/firecrest-v2/openapi).\n\n## Getting Started\n\n### Using the Python Interface\n\nOne way to get started is by using [pyFirecREST](https://pyfirecrest.readthedocs.io/), a Python package with a collection of wrappers for the different functionalities of FirecREST.\nThis package simplifies the usage of FirecREST by making multiple requests in the background for more complex workflows as well as by refreshing the access token before it expires.\n\n=== \"Version 2\"\n\n    ??? example \"Try FirecREST using pyFirecREST v2\"\n        ```python\n        import json\n        import firecrest as f7t\n\n        client_id = \"<client_id>\"\n        client_secret = \"<client_secret>\"\n        token_uri = \"https://auth.cscs.ch/auth/realms/firecrest-clients/protocol/openid-connect/token\"\n\n        # Setup the client for the specific account\n        # For instance, for the Alps HPC Platform system Daint:\n\n        client = f7t.v2.Firecrest(\n            firecrest_url=\"https://api.cscs.ch/hpc/firecrest/v2\",\n            authorization=f7t.ClientCredentialsAuth(client_id, client_secret, token_uri)\n        )\n\n        # Status of the systems, filesystems and schedulers:\n        print(json.dumps(client.systems(), indent=2))\n\n        # Output: information about systems and health status of the infrastructure\n        # [\n        #   {\n        #     \"name\": \"daint\",\n        #     \"ssh\": {                           # --> SSH settings\n        #       \"host\": \"daint.alps.cscs.ch\",\n        #       \"port\": 22,\n        #       \"maxClients\": 100,\n        #       \"timeout\": {\n        #         \"connection\": 5,\n        #         \"login\": 5,\n        #         \"commandExecution\": 5,\n        #         \"idleTimeout\": 60,\n        #         \"keepAlive\": 5\n        #       }\n        #     },\n        #     \"scheduler\": {                     # --> Scheduler settings\n        #       \"type\": \"slurm\",\n        #       \"version\": \"24.05.4\",\n        #       \"apiUrl\": null,\n        #       \"apiVersion\": null,\n        #       \"timeout\": 10\n        #     },\n        #     \"servicesHealth\": [                # --> Health status of services\n        #       {\n        #         \"serviceType\": \"scheduler\",\n        #         \"lastChecked\": \"2025-03-18T23:34:51.167545Z\",\n        #         \"latency\": 0.4725925922393799,\n        #         \"healthy\": true,\n        #         \"message\": null,\n        #         \"nodes\": {\n        #           \"available\": 21,\n        #           \"total\": 858\n        #         }\n        #       },\n        #       {\n        #         \"serviceType\": \"ssh\",\n        #         \"lastChecked\": \"2025-03-18T23:34:52.054056Z\",\n        #         \"latency\": 1.358715295791626,\n        #         \"healthy\": true,\n        #         \"message\": null\n        #       },\n        #       {\n        #         \"serviceType\": \"filesystem\",\n        #         \"lastChecked\": \"2025-03-18T23:34:51.969350Z\",\n        #         \"latency\": 1.2738196849822998,\n        #         \"healthy\": true,\n        #         \"message\": null,\n        #         \"path\": \"/capstor/scratch/cscs\"\n        #       },\n        #     (...)\n        #     \"fileSystems\": [                   # --> Filesystem settings\n        #       {\n        #         \"path\": \"/capstor/scratch/cscs\",\n        #         \"dataType\": \"scratch\",\n        #         \"defaultWorkDir\": true\n        #       },\n        #       {\n        #         \"path\": \"/users\",\n        #         \"dataType\": \"users\",\n        #         \"defaultWorkDir\": false\n        #       },\n        #       {\n        #         \"path\": \"/capstor/store/cscs\",\n        #         \"dataType\": \"store\",\n        #         \"defaultWorkDir\": false\n        #       }\n        #     ]    \n        #   }\n        # ]\n\n        # List content of directories\n        print(json.dumps(client.list_files(\"daint\", \"/capstor/scratch/cscs/<username>\"),\n                                        indent=2))\n        \n        # [\n        #   {\n        #     \"name\": \"directory\",\n        #     \"type\": \"d\",\n        #     \"linkTarget\": null,\n        #     \"user\": \"<username>\",\n        #     \"group\": \"<project>\",\n        #     \"permissions\": \"rwxr-x---+\",\n        #     \"lastModified\": \"2024-09-02T12:34:45\",\n        #     \"size\": \"4096\"\n        #   },\n        #   {\n        #     \"name\": \"file.txt\",\n        #     \"type\": \"-\",\n        #     \"linkTarget\": null,\n        #     \"user\": \"<username>\",\n        #     \"group\": \"<project>\",\n        #     \"permissions\": \"rw-r-----+\",\n        #     \"lastModified\": \"2024-09-02T08:26:04\",\n        #     \"size\": \"131225\"\n        #   }\n        # ]\n        ```\n\n=== \"Version 1\"\n\n    ??? example \"Try FirecREST using pyFirecREST v1\"\n        ```python\n        import json\n        import firecrest as f7t\n\n        client_id = \"<client_id>\"\n        client_secret = \"<client_secret>\"\n        token_uri = \"https://auth.cscs.ch/auth/realms/firecrest-clients/protocol/openid-connect/token\"\n\n        # Setup the client for the specific account\n        # For instance, for the Alps HPC Platform system Daint:\n\n        client = f7t.v1.Firecrest(\n            firecrest_url=\"https://api.cscs.ch/hpc/firecrest/v1\",\n            authorization=f7t.ClientCredentialsAuth(client_id, client_secret, token_uri)\n        )\n\n        print(json.dumps(client.all_systems(), indent=2))\n        # Output: (one dictionary per system)\n        # [{\n        #      'description': 'System ready',\n        #      'status': 'available',\n        #      'system': 'daint'      \n        # }]\n\n        print(json.dumps(client.list_files('daint', '/capstor/scratch/cscs/<username>'),\n                                        indent=2))\n        # Example output: (one dictionary per file)\n        # [\n        #   {\n        #       'name': 'directory',\n        #       'user': '<username>'\n        #       'last_modified': '2024-04-20T11:22:41',\n        #       'permissions': 'rwxr-xr-x',\n        #       'size': '4096',\n        #       'type': 'd',\n        #       'group': '<project>',\n        #       'link_target': '',\n        #   }\n        #   {\n        #      'name': 'file.txt',\n        #      'user': '<username>'\n        #      'last_modified': '2024-09-02T08:26:04',\n        #      'permissions': 'rw-r--r--',\n        #      'size': '131225',\n        #      'type': '-',\n        #      'group': '<project>',\n        #      'link_target': '',\n        #   }\n        # ]\n        ```\n\nThe tutorial is written for a generic instance of FirecREST but if you have a valid user at CSCS you can test it directly with your resource allocation on the exposed systems.\n\n### Data transfer with FirecREST\n\nIn addition to the [external transfer methods at CSCS][ref-data-xfer-external], FirecREST provides automated data transfer within the API.\n\nA staging area is used for external transfers and downloading/uploading a file from/to a CSCS filesystem.\n\n!!!Note\n    pyFirecREST (both v1 and v2) hides this complexity to the user. We strongly recommend to use this library for these tasks.\n\n=== \"Version 2\"\n\n    #### Upload\n    !!! example \"Upload a large file using FirecREST-v2\"\n        ```python\n\n        import firecrest as f7t\n\n        (...)\n\n        system = \"daint\"\n        source_path = \"/path/to/local/file\"\n        target_dir = \"/capstor/scratch/cscs/<username>\"\n        target_file = \"file\"\n        account = \"<project>\"\n\n\n        upload_task = client.upload(system,\n                                    local_file=source_path,\n                                    directory=target_dir,\n                                    filename=target_file,\n                                    account=account,\n                                    blocking=True)    \n        ```\n    #### Download\n\n    !!! example \"Download a large file using FirecREST-v2\"\n        ```python\n\n        import firecrest as f7t\n\n        (...)\n\n        system = \"daint\"\n        source_path = \"/capstor/scratch/cscs/<username>/file\"\n        target_path = \"/path/to/local/file\"\n        account = \"<project>\"\n\n\n        download_task = client.download(system,\n                                        source_path=source_path,\n                                        target_path=target_path,\n                                        account=account,\n                                        blocking=True)\n\n        \n        ```\n\n=== \"Version 1\"\n\n    Please follow the steps below to download a file:\n\n    1. Request FirecREST to move the file to the staging area: a download link will be provided\n    2. The file will remain in the staging area for 7 days or until the link gets invalidated with a request to the [`/storage/xfer-external/invalidate`](https://firecrest.readthedocs.io/en/latest/reference.html#post--storage-xfer-external-invalidate) endpoint or through the pyfirecrest method\n    3. The staging area is common for all users, therefore users should invalidate the link as soon as the download has been completed\n\n    You can see the full process in this [tutorial](https://firecrest.readthedocs.io/en/latest/tutorial.html#upload-with-non-blocking-call-something-bigger).\n\n    We may be forced to delete older files sooner than 7 days whenever large files are moved to the staging area and the link is not invalidated after the download, to avoid issues for other users: we will contact the user in this case.\n\n    When uploading files through the staging area, you don't need to invalidate the link. FirecREST will do it automatically as soon as it transfers the file to the filesystem of CSCS.\n\n    There is also a constraint on the size of a single file to transfer externally to our systems via FirecREST: 5 GB.\n\n    If you wish to transfer data bigger than the limit mentioned above, you can check the [compress](https://firecrest.readthedocs.io/en/latest/reference.html#post--storage-xfer-internal-compress) and [extract](https://firecrest.readthedocs.io/en/latest/reference.html#post--storage-xfer-internal-extract) endpoints or follow the following [example on how to split large files](https://github.com/eth-cscs/firecrest/blob/master/examples/download-large-files/README.md) and download/upload them using FirecREST.\n\n    The limit on the time and size of files that can be download/uploaded via FirecREST might change if needed.\n\n    !!! example \"Checking the current values in the parameters endpoint\"\n        ```python\n        >>> print(json.dumps(client.parameters(), indent = 2))\n        {\n        (...)\n\n        \"storage\": [\n            {\n            \"description\": \"Type of object storage, like `swift`, `s3v2` or `s3v4`.\",\n            \"name\": \"OBJECT_STORAGE\",\n            \"unit\": \"\",\n            \"value\": \"s3v4\"\n            },\n            {\n            \"description\": \"Expiration time for temp URLs.\",\n            \"name\": \"STORAGE_TEMPURL_EXP_TIME\",\n            \"unit\": \"seconds\",\n            \"value\": \"604800\"  ## <-------- 7 days\n            },\n            {\n            \"description\": \"Maximum file size for temp URLs.\",\n            \"name\": \"STORAGE_MAX_FILE_SIZE\",\n            \"unit\": \"MB\",\n            \"value\": \"5120\"   ## <--------- 5 GB\n            }\n        (...)\n        }\n        ```\n!!! Note \"Job submission through FirecREST\"\n\n    FirecREST provides an abstraction for job submission using in the backend the Slurm scheduler of the vCluster.\n\n    When submitting a job via the different [endpoints](https://firecrest.readthedocs.io/en/latest/reference.html#compute), you should pass the `-l` option to the `/bin/bash` command on the batch file.\n\n    ```bash\n    #!/bin/bash -l\n\n    #SBATCH --nodes=1\n    ...\n    ```\n\n    This option ensures that the job submitted uses the same environment as your login shell to access the system-wide profile (`/etc/profile`) or to your profile (in files like `~/.bash_profile`, `~/.bash_login`, or `~/.profile`).\n\n## Further Information\n\n* [HPC Platform Dashboard](https://my.hpcp.cscs.ch)\n* [ML Platform Dashboard](https://my.mlp.cscs.ch)\n* [C&W Platform Dashboard](https://my.cwp.cscs.ch)\n* [FirecREST OpenAPI Specification](https://eth-cscs.github.io/firecrest-v2/openapi)\n* [FirecREST Official Docs](https://eth-cscs.github.io/firecrest-v2)\n* [Documentation of pyFirecREST](https://pyfirecrest.readthedocs.io/)\n* [FirecREST repository](https://github.com/eth-cscs/firecrest-v2)\n* [What are JSON Web Tokens](https://jwt.io/introduction)\n* [Python Requests](https://requests.readthedocs.io/en/master/user/quickstart)\n* [Python Async API Calls](https://docs.aiohttp.org/en/stable/)\n",
    "source": "cscs-docs"
  },
  {
    "doc_id": 2,
    "file_path": "access\\hpc-console.md",
    "file_name": "hpc-console.md",
    "title": "HPC Console",
    "content": "[](){#hpc-console}\n# HPC Console\n\nHPC Console is a web application that provides users with a modern web-based environment to interact with HPC resources such as clusters, job schedulers, and filesystems.\n\n## Features\n\n\n- Dashboard – shows cluster availability and basic system status.\n- Jobs – allows creating, submitting, and monitoring jobs. Users can provide job name, script, and resource requirements.\n- File Navigator – supports browsing directories, uploading, downloading, and deleting files. Editing files inline is not supported.\n\n## Alps platforms\n\nHPC console is currently available on three [Alps platforms][ref-alps-platforms].\n\n| Platform       | URL               | Clusters                                                                 |\n|----------------|----------------------|--------------------------------------------------------------------------|\n| HPC Platform   | [https://my.hpcp.cscs.ch/](https://my.hpcp.cscs.ch/) | [Daint][ref-cluster-daint], [Eiger][ref-cluster-eiger]             |\n| ML Platform    | [https://my.mlp.cscs.ch/](https://my.mlp.cscs.ch/)  | [Bristen][ref-cluster-bristen], [Clariden][ref-cluster-clariden]  |\n| CW Platform    | [https://my.cwp.cscs.ch/](https://my.cwp.cscs.ch/)  | [Santis][ref-cluster-santis]  |\n\n## Access and prerequisites\n\n- Access requires a valid CSCS account and authentication through the institutional login system.\n- Supported browsers: recent versions of Chrome, Firefox, and Safari are recommended.\n- Sessions automatically expire after a period of inactivity.\n\n## Feedback\n\nWe welcome your feedback to help us improve HPC Console.\nIf you encounter issues or have suggestions, please [let us know][ref-get-in-touch].\n\n## Further information\n\n* [FirecREST UI documentation](https://eth-cscs.github.io/firecrest-ui)\n* [FirecREST UI repository](https://github.com/eth-cscs/firecrest-ui)\n* [FirecREST documentation](https://eth-cscs.github.io/firecrest-v2)",
    "source": "cscs-docs"
  },
  {
    "doc_id": 3,
    "file_path": "access\\index.md",
    "file_name": "index.md",
    "title": "Connecting to Alps",
    "content": "# Connecting to Alps\n\nThis documentation guides users through the process of accessing CSCS systems and services.\n\n!!! note \"\"\n    Before accessing CSCS, you need to have an account at CSCS, and be part of a project that has been allocated resources.\n    More information on how to get an account is available in [accounts and projects][ref-account-management].\n\n<div class=\"grid cards\" markdown>\n\n-   :fontawesome-solid-layer-group: __Multi Factor Authentication__\n\n    Before signing in to CSCS' web portals or using SSH, all users have to set up multi factor authentication (MFA)\n\n    [:octicons-arrow-right-24: MFA][ref-mfa]\n\n-   :fontawesome-solid-layer-group: __Web Services__\n\n    Before signing in to CSCS' web portals or using SSH, all users have to set up multi factor authentication (MFA)\n\n    [:octicons-arrow-right-24: Accessing CSCS web services][ref-access-web]\n\n-   :fontawesome-solid-layer-group: __SSH Access__\n\n    Logging into Clusters on Alps\n\n    [:octicons-arrow-right-24: SSH][ref-ssh]\n\n-   :material-fire-circle: __FirecREST__\n\n    FirecREST is a RESTful API for programmatically accessing High-Performance Computing resources.\n\n    [:octicons-arrow-right-24: FirecREST][ref-firecrest]\n\n-   :simple-jupyter: __JupyterLab__\n\n    JupyterLab is a feature-rich notebook authoring application and editing environment.\n\n    [:octicons-arrow-right-24: JupyterLab][ref-jupyter]\n\n-   :fontawesome-solid-layer-group: __VSCode__\n\n    How to connect VSCode IDE on your laptop with Alps\n\n    [:octicons-arrow-right-24: SSH][ref-access-vscode]\n\n</div>\n",
    "source": "cscs-docs"
  },
  {
    "doc_id": 4,
    "file_path": "access\\jupyterlab.md",
    "file_name": "jupyterlab.md",
    "title": "JupyterLab",
    "content": "[](){#ref-jupyter}\n# JupyterLab\n\n## Access and setup\n\nThe JupyterHub service enables the interactive execution of JupyterLab on the compute nodes of [Daint][ref-cluster-daint], [Clariden][ref-cluster-clariden], [Santis][ref-cluster-santis] and [Eiger][ref-cluster-eiger].\n\nThe service is accessed at [jupyter-daint.cscs.ch](https://jupyter-daint.cscs.ch/), [jupyter-clariden.cscs.ch](https://jupyter-clariden.cscs.ch/), [jupyter-santis.cscs.ch](https://jupyter-clariden.cscs.ch/) and [jupyter-eiger.cscs.ch](https://jupyter-eiger.cscs.ch), respectively. As the notebook servers are executed on compute nodes, you must have a project with compute resources available on the respective cluster.\n\nOnce logged in, you will be redirected to the JupyterHub Spawner Options form, where typical job configuration options can be selected. These options might include the type and number of compute nodes, the wall time limit, and your project account.\n\nBy default, JupyterLab servers are launched in a dedicated queue, which should ensure a start-up time of less than a few minutes. If your server is not running within 5 minutes we encourage you to first try the non-dedicated queue, and then [contact us][ref-get-in-touch].\n\nWhen resources are granted the page redirects to the JupyterLab session, where you can browse, open and execute notebooks on the compute nodes. A new notebook with a Python 3 kernel can be created with the menu `new` and then `Python 3` . Under `new` it is also possible to create new text files and folders, as well as to open a terminal session on the allocated compute node.\n\n!!! tip \"Debugging\"\n    The log file of a JupyterLab server session is saved in `$HOME` in a file named `slurm-<jobid>.out`. If you encounter problems with your JupyterLab session, the contents of this file can contain clues to debug the issue.\n\n??? warning \"Unexpected error while saving file: disk I/O error.\"\n    This error message indicates that you have run out of disk quota.\n    You can check your quota using the command `quota`.\n\n[](){#ref-jupyter-runtime-environment}\n## Runtime environment\n\nA Jupyter session can be started with either a [uenv][ref-uenv] or a [container][ref-container-engine] as a base image. The JupyterHub Spawner form provides a set of default images such as the [prgenv-gnu][ref-uenv-prgenv-gnu] uenv or the [NGC PyTorch container][ref-software-ml] to choose from in a dropdown menu. When using uenv, the software stack will be mounted at `/user-environment`, and the specified view will be activated. For a container, the Jupyter session will launch inside the container filesystem with only a select set of paths mounted from the host. Once you have found a suitable option, you can start the session with `Launch JupyterLab`.\n\n??? info \"Using remote uenv for the first time.\"\n    If the uenv is not present in the local repository, it will be automatically fetched.\n    As a result, JupyterLab may take slightly longer than usual to start.\n\n!!! warning \"Ending your interactive session and logging out\"\n    The Jupyter servers can be shut down through the Hub. To end a JupyterLab session, please select `Hub Control Panel` under the `File` menu and then `Stop My Server`. By contrast, clicking `Logout` will log you out of the server, but the server will continue to run until the Slurm job reaches its maximum wall time.\n\nIf the default base images do not meet your requirements, you can specify a custom environment instead. For this purpose, you supply either a custom uenv image/view or [container engine (CE)][ref-container-engine] TOML file under the section `Advanced options` before launching the session. The supported uenvs are compatible with the Jupyter service out of the box, whereas container images typically require the installation of some additional packages. \n\n??? \"Example of a custom PyTorch container\"\n    A container image based on recent a NGC PyTorch release requires the installation of the following additional packages to be compatible with the Jupyter service:\n\n    ```Dockerfile\n    FROM nvcr.io/nvidia/pytorch:25.05-py3\n\n    RUN pip install --no-cache \\\n        jupyterlab \\\n        jupyterhub==4.1.6 \\\n        pyfirecrest==1.2.0 \\\n        SQLAlchemy==1.4.52 \\\n        oauthenticator==16.3.1 \\\n        notebook==7.3.3 \\\n        jupyterlab_nvdashboard==0.13.0 \\\n        git+https://github.com/eth-cscs/firecrestspawner.git\n    ```\n\n    The package [nvdashboard](https://github.com/rapidsai/jupyterlab-nvdashboard) is also installed here, which allows to monitor system metrics at runtime.\n    \n    A corresponding TOML file can look like\n\n    ```toml\n    image = \"/capstor/scratch/cscs/${USER}/ce-images/ngc-pytorch+25.05.sqsh\"\n\n    mounts = [\n        \"/capstor\", \n        \"/iopsstor\",\n        \"/users/${USER}/.local/share/jupyter\", # (1)!\n        \"/etc/slurm\", # (2)!\n        \"/usr/lib64/libslurm-uenv-mount.so\",\n        \"/etc/container_engine_pyxis.conf\" # (3)!\n    ]\n\n    workdir = \"/capstor/scratch/cscs/${USER}\" # (4)!\n\n    writable = true\n\n    [annotations]\n    com.hooks.aws_ofi_nccl.enabled = \"true\" # (5)!\n    com.hooks.aws_ofi_nccl.variant = \"cuda12\"\n\n    [env]\n    CUDA_CACHE_DISABLE = \"1\" # (6)!\n    TORCH_NCCL_ASYNC_ERROR_HANDLING = \"1\" # (7)!\n    MPICH_GPU_SUPPORT_ENABLED = \"0\" # (8)!\n    ```\n    \n    1. Avoid mounting all of `$HOME` to avoid subtle issues with cached files, but mount Jupyter kernels\n    2. Enable Slurm commands (together with two subsequent mounts)\n    3. Required only for Daint and Santis; Do not use on Clariden\n    4. Set working directory of Jupyter session (file browser root directory)\n    5. Use environment settings for optimized communication \n    6. Avoid writing JITed binaries to the (distributed) file system, which could lead to performance issues.\n    7. Async error handling when an exception is observed in NCCL watchdog: aborting NCCL communicator and tearing down process upon error\n    8. Disable GPU support in MPICH, as it can lead to deadlocks when using together with NCCL\n\n??? tip \"Accessing file systems with uenv\"\n    While Jupyter sessions with CE start in the directory specified with `workdir`, a uenv session always start in your `$HOME` folder. All non-hidden files and folders in `$HOME` are visible and accessible through the JupyterLab file browser. However, you can not browse directly to folders above `$HOME`. To enable access your `$SCRATCH` folder, it is therefore necessary to create a symbolic link to your `$SCRATCH` folder. This can be done by issuing the following command in a terminal from your `$HOME` directory:\n    ```bash\n    ln -s $SCRATCH $HOME/scratch\n    ```\n\n## Creating Jupyter kernels\n\nA kernel, in the context of Jupyter, is a program together with environment settings that runs the user code within Jupyter notebooks. In Python, Jupyter kernels make it possible to access the (system) Python installation of a uenv or container, that of a virtual environment (on top) or any other custom Python installations like Anaconda/Miniconda from Jupyter notebooks. Alternatively, a kernel can also be created for other programming languages such as Julia, allowing e.g. the execution of Julia code in notebook cells. \n\nAs a preliminary step to running any code in Jupyter notebooks, a kernel needs to be installed, which is described in the following for both Python and Julia.\n\n### Using Python in Jupyter\n\nFor Python, the recommended setup consists of a uenv or container as a base image as described [above][ref-jupyter-runtime-environment] that includes the stable dependencies of the software stack. Additional packages can be installed in a virtual environment _on top_ of the Python installation in the base image (mandatory for most uenvs). Having the base image loaded, such a virtual environment can be created with\n\n```bash title=\"Create a virtual environment on top of a base image\"\npython -m venv --system-site-packages venv-<base-image-version>\n```\n\nwhere `<base-image-version>` can be replaced by an identifier uniquely referring to the base image (such virtual environments are specific for the base image and are not portable).\n\nJupyter kernels for Python are powered by [`ipykernel`](https://github.com/ipython/ipykernel).\nAs a result, `ipykernel` must be installed in the target environment that will be used as a kernel. That can be done with `pip install ipykernel` (either as part of a Dockerfile or in an activated virtual environment on top of a uenv/container image).\n\nA kernel can now be created from an active Python virtual environment with the following commands\n\n```bash title=\"Create an IPython Jupyter kernel\"\n. venv-<base-image-version>/bin/activate # (1)!\npython -m ipykernel install \\\n    ${VIRTUAL_ENV:+--env PATH $PATH --env VIRTUAL_ENV $VIRTUAL_ENV} \\\n    --user --name=\"<kernel-name>\" # (2)!\n```\n\n1. This step is only necessary when working with a virtual environment on top of the base image\n2. The expression in braces makes sure the kernel's environment is properly configured when using a virtual environment (must be activated). The flag `--user` installs the kernel to a path under `${HOME}/.local/share/jupyter`. \n\nThe `<kernel-name>` can be replaced by a name specific to the base image/virtual environment.\n\n??? bug \"Python packages from uenv shadowing those in a virtual environment\"\n    When using uenv with a virtual environment on top, the site-packages under `/user-environment` currently take precedence over those in the activated virtual environment. This is due to the uenv paths being included in the `PYTHONPATH` environment variable. As a consequence, despite installing a different version of a package in the virtual environment from what is available in the uenv, the uenv version will still be imported at runtime. A possible workaround is to prepend the virtual environment's site-packages to `PYTHONPATH` whenever activating the virtual environment.\n    ```bash\n    export PYTHONPATH=\"$(python -c 'import site; print(site.getsitepackages()[0])'):$PYTHONPATH\"\n    ```\n    Consequently, a modified command should be used to install the Jupyter kernel that carries over the changed `PYTHONPATH` to the Jupyter environment. This can be done as follows.\n    ```bash\n    python -m ipykernel install \\\n        ${VIRTUAL_ENV:+--env PATH $PATH --env VIRTUAL_ENV $VIRTUAL_ENV ${PYTHONPATH+--env PYTHONPATH $PYTHONPATH}} \\\n        --user --name=\"<kernel-name>\"\n    ```\n    It is recommended to apply this workaround if you are constrained by a Python package version installed in the uenv that you need to change for your application.\n\n### Using Julia in Jupyter\n\nTo run Julia code in Jupyter notebooks, you can use the provided uenv for this language. In particular, you need to use the following in the JupyterHub Spawner `Advanced options` forms mentioned [above][ref-jupyter-runtime-environment]:\n!!! important \"pass a [`julia`][ref-uenv-julia] uenv and the view `jupyter`.\"\n\nWhen Julia is first used within Jupyter, IJulia and one or more Julia kernel need to be installed. \nType the following command in a shell within JupyterHub to install IJulia, the default Julia kernel and, on systems with Nvidia GPUs, a Julia kernel running under Nvidia Nsight Systems:\n```bash\ninstall_ijulia\n```\n\nYou can install additional custom Julia kernels by typing the following in a shell:\n```bash\njulia\nusing IJulia\ninstallkernel(<args>) # (1)!\n```\n\n1. type `? installkernel` to learn about valid `<args>`\n\n!!! warning \"First time use of Julia\"\n    If you are using Julia for the first time at all, executing `install_ijulia` will automatically first trigger the installation of `juliaup` and the latest `julia` version (it is also triggered if you execute `juliaup` or `julia`).\n\n## Parallel computing\n\n### MPI in the notebook via IPyParallel and MPI4Py\n\nMPI for Python provides bindings of the Message Passing Interface (MPI) standard for Python, allowing any Python program to exploit multiple processors.\n\nMPI can be made available on Jupyter notebooks through [IPyParallel](https://github.com/ipython/ipyparallel). This is a Python package and collection of CLI scripts for controlling clusters for Jupyter: a set of servers that act as a cluster, called engines, is created and the code in the notebook's cells will be executed within them.\n\nWe provide the Python package [`ipcmagic`](https://github.com/eth-cscs/ipcluster_magic) to make easier the management of IPyParallel clusters. `ipcmagic` can be installed by the user with\n\n```bash\npip install ipcmagic-cscs\n```\n\nThe engines and another server that moderates the cluster, called the controller, can be started an stopped with the magic `%ipcluster start -n <num-engines>` and `%ipcluster stop`, respectively. Before running the command, the python package `ipcmagic` must be imported\n\n```bash\nimport ipcmagic\n```\n\nInformation about the command, can be obtained with `%ipcluster --help`.\n\nIn order to execute MPI code on JupyterLab, it is necessary to indicate that the cells have to be run on the IPyParallel engines. This is done by adding the [IPyParallel magic command](https://ipyparallel.readthedocs.io/en/latest/tutorial/magics.html) `%%px` to the first line of each cell.\n\nThere are two important points to keep in mind when using IPyParallel. The first one is that the code executed on IPyParallel engines has no effect on non-`%%px` cells. For instance, a variable created on a `%%px`-cell will not exist on a non-`%%px`-cell. The opposite is also true. A variable created on a regular cell, will be unknown to the IPyParallel engines. The second one is that the IPyParallel engines are common for all the user's notebooks. This means that variables created on a `%%px` cell of one notebook can be accessed or modified by a different notebook.\n\nThe magic command `%autopx` can be used to make all the cells of the notebook `%%px`-cells. `%autopx` acts like a switch: running it once, activates the `%%px` and running it again deactivates it. If `%autopx` is used, then there are no regular cells and all the code will be run on the IPyParallel engines.\n\nExamples of notebooks with `ipcmagic` can be found [here](https://github.com/eth-cscs/ipcluster_magic/tree/master/examples).\n\n### Distributed training and inference for ML\n\nWhile it is generally recommended to submit long-running machine learning training and inference jobs via `sbatch`, certain use cases can benefit from an interactive Jupyter environment.\n\nA popular approach to run multi-GPU ML workloads is with [`accelerate`](https://github.com/huggingface/accelerate) and [`torchrun`](https://docs.pytorch.org/docs/stable/elastic/run.html) as demonstrated in the [tutorials][ref-tutorials-ml].\nIn particular, the `accelerate launch` script in the [LLM fine-tuning tutorial][software-ml-llm-fine-tuning-tutorial] can be directly carried over to a Jupyter cell with a `%%bash` header (to run its contents interpreted by bash).\nFor `torchrun`, one can adapt the command from the multi-node [nanotron tutorial][software-ml-llm-nanotron-tutorial] to run on a single GH200 node using the following line in a Jupyter cell\n\n```bash\n!python -m torch.distributed.run --standalone --nproc_per_node=4 run_train.py ...\n```\n\n!!! warning \"torchrun with virtual environments\"\n    When using a virtual environment on top of a base image with PyTorch, always replace `torchrun` with `python -m torch.distributed.run` to pick up the correct Python environment. Otherwise, the system Python environment will be used and virtual environment packages will not available. If not using virtual environments such as with a self-contained PyTorch container, `torchrun` is equivalent to `python -m torch.distributed.run`.\n\n!!! note \"Notebook structure\"\n    In none of these scenarios any significant memory allocations or background computations are performed on the main Jupyter process. Instead, the resources are kept available for the processes launched by `accelerate` or `torchrun`, respectively.\n\nAlternatively to using these launchers, it is also possible to use Slurm to obtain more control over resource mappings, e.g. by launching an overlapping Slurm step onto the same node used by the Jupyter process. An example with the container engine looks like this:\n\n```bash\n!srun --overlap -ul --environment /path/to/edf.toml \\\n    --container-workdir $PWD -n 4 bash -c \"\\\n    . venv-<base-image-version>/bin/activate\n    MASTER_ADDR=\\$(scontrol show hostnames \\$SLURM_JOB_NODELIST | head -n 1) \\\n    MASTER_PORT=29500 \\\n    RANK=\\$SLURM_PROCID LOCAL_RANK=\\$SLURM_LOCALID WORLD_SIZE=\\$SLURM_NPROCS \\\n    python train.py ...\"\n```\n\nwhere `/path/to/edf.toml` should be replaced by the TOML file and `venv-<base-image-version>` by the name of the virtual environment (if used). The script `train.py` is using `torch.distributed` for distributed training. This launch mechanism can be further customized with extra Slurm options.\n\n!!! warning \"Concurrent usage of resources\"\n    Subtle bugs can occur when running multiple Jupyter notebooks concurrently that each assume access to the full node. Also, some notebooks may hold on to resources such as spawned child processes or allocated memory despite having completed. In this case, resources such as a GPU may still be busy, blocking another notebook from using it. Therefore, it is good practice to only keep one such notebook running that occupies the full node and restarting a kernel once a notebook has completed. If in doubt, system monitoring with `htop` and [nvdashboard](https://github.com/rapidsai/jupyterlab-nvdashboard) can be helpful for debugging.\n\n!!! warning \"Multi-GPU training from a shared Jupyter process\"\n    Running multi-GPU training workloads directly from the shared Jupyter process is generally not recommended due to potential inefficiencies and correctness issues (cf. the [PyTorch docs](https://docs.pytorch.org/docs/stable/notes/cuda.html#use-nn-parallel-distributeddataparallel-instead-of-multiprocessing-or-nn-dataparallel)). However, if you need it to e.g. reproduce existing results, it is possible to do so with utilities like `accelerate`'s `notebook_launcher` or [`transformers`](https://github.com/huggingface/transformers)' `Trainer` class. When using these in containers, you will currently need to unset the environment variables `RANK` and `LOCAL_RANK` by adding the following in a cell at the top of the notebook:\n\n    ```python\n    import os; os.environ.pop(\"RANK\"); os.environ.pop(\"LOCAL_RANK\");\n    ```\n\n## Further documentation\n\n* [Jupyter](http://jupyter.org/)\n* [JupyterLab](https://jupyterlab.readthedocs.io/en/stable/)\n* [JupyterHub](https://jupyterhub.readthedocs.io/en/stable)\n",
    "source": "cscs-docs"
  }
]