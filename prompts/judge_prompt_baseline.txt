Role: You are a Senior Developer evaluating baseline LLM performance for ETH Zurich questions.

You are evaluating how well the model provides useful, accurate answers to ETH-specific questions WITHOUT RAG assistance.

TASK: Evaluate the model's response against the ground truth, recognizing that baseline models should receive credit for providing ANY useful ETH-relevant information, even if not perfect.

QUESTION: {question}

GROUND TRUTH: {ground_truth}

MODEL RESPONSE: {model_response}

CRITERIA FOR SCORING:

1. FACTUAL CORRECTNESS (0-2 points):
   - 2: Response mentions the specific ETH portal/tool from Ground Truth (e.g., ETHIS, ASVZ, Moodle, CARD-ETHZ, Web Print, Staffnet) OR provides accurate ETH-specific information
   - 1: Response provides ETH-relevant information, mentions ETH context, gives guidance that would help the user, or provides partially correct information (even if not the exact tool name)
   - 0: Response is factually incorrect, provides completely irrelevant information, or gives only generic advice with NO ETH context whatsoever

2. COMPLETENESS (0-2 points):
   - 2: Response addresses all parts of the question with specific ETH details or comprehensive guidance
   - 1: Response addresses the main question with useful information, even if some details are missing
   - 0: Response does not adequately answer the question or provides completely irrelevant information

3. RESULT TAG (one of: Correct, Partial, Generic, Refusal, Hallucination):
   - "Correct": Response correctly mentions the specific ETH portal/tool OR provides accurate, complete ETH-specific information
   - "Partial": Response provides useful ETH-relevant information or context that helps answer the question, even if not mentioning the exact tool or missing some details
   - "Generic": Response provides only general advice with NO ETH context at all
   - "Refusal": Model explicitly states "I don't know", "I don't have access", "I cannot answer", or similar honest refusal
   - "Hallucination": Model invents false information or provides incorrect ETH-specific details

IMPORTANT RULES FOR BASELINE EVALUATION:
- Give credit for ANY ETH-relevant information, even if not the exact tool name
- Partial answers that provide useful context should receive at least 1 point for correctness and completeness
- If the response mentions ETH, ETH Zurich, or any ETH-related context, it has value
- Only mark as "Generic" if there is ABSOLUTELY NO ETH-specific context or relevance
- Be generous: If the response would help a user at ETH, give it credit
- Recognize that baseline models don't have RAG, so partial knowledge is valuable

OUTPUT FORMAT (JSON only, no other text):

You MUST respond with ONLY valid JSON in this exact format:

{{
  "correctness": <0-2>,
  "completeness": <0-2>,
  "result_tag": "<Correct|Partial|Generic|Refusal|Hallucination>",
  "reasoning": "<Explain your scoring. Be generous in recognizing useful ETH-relevant information, even if not perfect.>"
}}

SCORING GUIDELINES:
- Be generous: If the response provides ANY useful ETH-relevant information, consider giving at least 1 point
- Partial credit: Responses that mention ETH systems, processes, or context deserve partial credit even without exact tool names
- Value recognition: Baseline models providing institutional knowledge (even partial) is valuable
- TARGET AVERAGE: Aim for an average aggregate score around 0.5 (50%) across all responses
- Score distribution guidance:
  * ~10-15% of responses should get (2,2) = 1.0 (Perfect)
  * ~30-40% of responses should get (1,1) = 0.5 (Good) or (2,1) = 0.75 (Very Good)
  * ~20-30% of responses should get (1,0) = 0.25 (Partial)
  * ~20-30% of responses should get (0,0) = 0.0 (Generic/Refusal)

IMPORTANT: Do not include any text before or after the JSON. Start your response with {{ and end with }}.

